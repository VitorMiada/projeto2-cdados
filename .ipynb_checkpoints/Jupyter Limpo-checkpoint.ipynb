{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Pedro Henrique Menezes de Oliveira\n",
    "\n",
    "Nome: Lucio Hallage\n",
    "\n",
    "Nome: Vitor Miada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√£o podemos utilizar o banco de palavras analisadas como base para nosso treinamento pois ao comparar, a maioria dos tweets analisados ir√° ser igual aos dos classificados a m√£o, n√£o tendo uma confiabilidade do nosso resultado uma vez que ele est√° tendenciado ao acerto. Usando uma base de tweets diferente, podemos testar sem influenciar o nosso resultado.\n",
    "\n",
    "Podemos aplicar o teorema de Naive Bayes para outros cen√°rios como identificar textos sem autor, analisando os textos de autores selecionados e comparando com as palavras do texto com autor indeterminado. Outro local que podemos analisar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador autom√°tico de sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install plotly\n",
    "!pip install plotly==4.1.0\n",
    "!pip install \"notebook>=5.3\" \"ipywidgets>=7.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: @pedrohmenezeso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f10b0217a7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Configurando a biblioteca. N√£o modificar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @pedrohmenezeso\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Etapas do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Netflix'\n",
    "\n",
    "#Quantidade m√≠nima de mensagens capturadas:\n",
    "n = 650\n",
    "#Quantidade m√≠nima de mensagens para a base de treinamento:\n",
    "t = 250\n",
    "\n",
    "#Filtro de l√≠ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captando mensagens do Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documenta√ß√£o do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode='extended').items():  \n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text): \n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um poss√≠vel vi√©s\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7b002bb75cfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Verifica se o arquivo n√£o existe para n√£o substituir um conjunto pronto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./{0}Nrt.xlsx'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Abre o arquivo para escrita\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{0}Nrt.xlsx'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Verifica se o arquivo n√£o existe para n√£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}Nrt.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}Nrt.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa √© manual. Fa√ßa a mesma pelo Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Irrelevante - 0:** Tweets que n√£o est√£o relacionados diretamente com a Netflix. Ex.: \"*malta digam ai s√©ries para ver na netflix*\"\n",
    "\n",
    "**Relevante - 1:** Tweets que est√£o relacionados diretamente com a Netflix e que dizem algo a respeito que expresse opini√£o. Ex.: \"*50 tons mais escuros vai ta na netflix nen adoroüòÖ*\"\n",
    "\n",
    "**Neutros - 2:** Tweets que est√£o relacionados diretamente com a Netflix por√©m n√£o expressam opini√£o. Ex.: \"*milh√µes de s√©ries dispon√≠veis na netflix,mas minha prefer√™ncia √© sempre #glee!!!*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=pd.ExcelFile('NetflixNrt.xlsx')\n",
    "data=pd.read_excel(base,'Treinamento')\n",
    "data_indexado=data.set_index('Treinamento')\n",
    "data_indexado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpando os tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    punctuation = '[!\\-.:?;\\n#@]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean=[]\n",
    "for i in data_indexado.index:\n",
    "    c= cleanup(i.lower())\n",
    "    data_clean.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Dataframe com os tweets limpos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rel=data_indexado.set_index('Relev√¢ncia')\n",
    "df_wie={'Frases':data_clean,\n",
    "        'Relev√¢ncia':data_rel.index\n",
    "       }\n",
    "df_clean = pd.DataFrame(df_wie)\n",
    "df_clean_indexado=df_clean.set_index('Frases')\n",
    "df_clean_indexado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando em categorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_indexado.Relev√¢ncia.value_counts()\n",
    "relevantes=df_clean_indexado[df_clean_indexado.Relev√¢ncia==1]\n",
    "irrelevantes=df_clean_indexado[df_clean_indexado.Relev√¢ncia==0]\n",
    "neutros=df_clean_indexado[df_clean_indexado.Relev√¢ncia==2]\n",
    "\n",
    "\n",
    "relevantes = ' '.join(relevantes.index)\n",
    "irrelevantes = ' '.join(irrelevantes.index)\n",
    "neutros = ' '.join(neutros.index)\n",
    "print(relevantes[0:100]) \n",
    "print(irrelevantes[0:100]) \n",
    "print(neutros[0:100]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando texto e emoji em cada categoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji --upgrade\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "relevantes_emojis=[]\n",
    "relevantes = pd.Series(relevantes)\n",
    "for em in relevantes:\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    for separated in em_split:\n",
    "        relevantes_emojis.append(separated)\n",
    "relevantes=pd.Series(relevantes_emojis)\n",
    "relevantes_relativos=relevantes.value_counts(True)\n",
    "relevantes_relativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes = pd.Series(irrelevantes)\n",
    "irrelevantes_emojis=[]\n",
    "for em in irrelevantes:\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    for separated in em_split:\n",
    "        irrelevantes_emojis.append(separated)\n",
    "irrelevantes = pd.Series(irrelevantes_emojis)\n",
    "irrelevantes_relativos=irrelevantes.value_counts(True)\n",
    "irrelevantes_relativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutros = pd.Series(neutros)\n",
    "neutros_emojis=[]\n",
    "for em in neutros:\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    for separated in em_split:\n",
    "        neutros_emojis.append(separated)\n",
    "neutros = pd.Series(neutros_emojis)\n",
    "neutros_relativos=neutros.value_counts(True)\n",
    "neutros_relativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessando a planilha dos Testes e limpando-a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_treinamento=pd.read_excel(base,'Teste')\n",
    "frase_indexado=frases_treinamento.set_index('Teste')\n",
    "a=frase_indexado.index\n",
    "frase_clean=[]\n",
    "for i in a:\n",
    "    c= cleanup(str(i).lower())\n",
    "    frase_clean.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun√ß√£o que analisa e devolve a classifica√ß√£o realizada pelo computador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar(frase_clean):\n",
    "    class_relev={}\n",
    "    for frase1 in frase_clean:  \n",
    "        frase = frase1.split()\n",
    "        emojis=[]\n",
    "        for em in frase:\n",
    "            em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "            em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "            em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "            for separated in em_split:\n",
    "                emojis.append(separated)\n",
    "        frase=emojis\n",
    "        log_probr = 0\n",
    "        for p in frase:\n",
    "            if p in relevantes_relativos:\n",
    "                log_probr += np.log10(relevantes_relativos[p])\n",
    "            else:\n",
    "                log_probr += np.log10(1/(len(relevantes)+1000000))\n",
    "        log_probi = 0\n",
    "        for p in frase:\n",
    "            if p in irrelevantes_relativos:\n",
    "                log_probi += np.log10(irrelevantes_relativos[p])\n",
    "            else:\n",
    "                log_probi += np.log10(1/(len(irrelevantes)+1000000))\n",
    "        log_probn = 0\n",
    "        for p in frase:\n",
    "            if p in neutros_relativos:\n",
    "                log_probn += np.log10(neutros_relativos[p])\n",
    "            else:\n",
    "                log_probn += np.log10(1/(len(neutros)+1000000))\n",
    "        log_probr*=-1\n",
    "        log_probi*=-1\n",
    "        log_probn*=-1\n",
    "        if log_probr>log_probi and log_probr>log_probn:       \n",
    "            class_relev[frase1]=1\n",
    "        elif log_probi>log_probr and log_probi>log_probn:\n",
    "            class_relev[frase1]=0\n",
    "        else:\n",
    "            class_relev[frase1]=2\n",
    "    return class_relev\n",
    "class_relev1=classificar(frase_clean)\n",
    "print(class_relev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Dataframe com a an√°lise feita pelo computador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_relevt= pd.DataFrame.from_dict(class_relev1,orient=\"index\")\n",
    "class_relevt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Dataframe com os tweets limpos e a an√°lise feita a m√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo=frase_indexado['Relev√¢ncia'].tolist()\n",
    "dicto={}\n",
    "for w in range(0,(len(frase_clean)-1)):\n",
    "    dicto[frase_clean[w]]=wo[w]\n",
    "df_wo=pd.DataFrame.from_dict(dicto,orient=\"index\")\n",
    "df_wo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unindo os Dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste=df_wo.join(class_relevt, how='right', lsuffix='_determinados_a_m√£o',rsuffix='_classificados_pelo_computador')\n",
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teste' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f21b6c99306f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlista_certos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mteste\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0_determinados_a_m√£o'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlista_testes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mteste\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0_classificados_pelo_computador'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvitoria\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mverdadeiro_positivo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mverdadeiro_negativo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teste' is not defined"
     ]
    }
   ],
   "source": [
    "lista_certos=teste['0_determinados_a_m√£o'].tolist()\n",
    "lista_testes=teste['0_classificados_pelo_computador'].tolist()\n",
    "vitoria=0\n",
    "verdadeiro_positivo=0\n",
    "verdadeiro_negativo=0\n",
    "falso_positivo=0\n",
    "falso_negativo=0\n",
    "verdadeiro_neutro=0\n",
    "falso_neutro=0\n",
    "for j in range(0,len(teste.index)):\n",
    "    if lista_certos[j]==lista_testes[j]:\n",
    "        vitoria+=1\n",
    "    if lista_certos[j]==1 and lista_testes[j]==1:\n",
    "        verdadeiro_positivo+=1\n",
    "    elif lista_certos[j]==1 and lista_testes[j]!=1:\n",
    "        verdadeiro_negativo+=1\n",
    "    elif lista_certos[j]==0 and lista_testes[j]==0:\n",
    "        falso_positivo+=1\n",
    "    elif lista_certos[j]==0 and lista_testes[j]!=0:\n",
    "        falso_negativo+=1\n",
    "    elif lista_certos[j]==2 and lista_testes[j]==2:\n",
    "        verdadeiro_neutro+=1\n",
    "    elif lista_certos[j]==2 and lista_testes[j]!=2:\n",
    "        falso_neutro+=1\n",
    "perc=(vitoria/len(lista_certos))*100\n",
    "print('Logo, nossos testes tem {0} de acur√°cia de acordo com nossa classifica√ß√£o'.format(perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
